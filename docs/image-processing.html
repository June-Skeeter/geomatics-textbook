<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Image Processing | An Open Geomatics Textbook</title>
  <meta name="description" content="Advancing teaching and learning in geomatics at UBC" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Image Processing | An Open Geomatics Textbook" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/ubc-geomatics-textbook/book/" />
  
  <meta property="og:description" content="Advancing teaching and learning in geomatics at UBC" />
  <meta name="github-repo" content="ubc-geomatics-textbook/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Image Processing | An Open Geomatics Textbook" />
  
  <meta name="twitter:description" content="Advancing teaching and learning in geomatics at UBC" />
  

<meta name="author" content="UBC" />


<meta name="date" content="2021-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fundamentals-of-remote-sensing.html"/>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Open Geomatics Textbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#contacts"><i class="fa fa-check"></i><b>0.1</b> Contacts</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#project-wiki"><i class="fa fa-check"></i><b>0.2</b> Project Wiki</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#style-guide"><i class="fa fa-check"></i><b>0.3</b> Style Guide</a>
<ul>
<li class="chapter" data-level="0.3.1" data-path="index.html"><a href="index.html#audience"><i class="fa fa-check"></i><b>0.3.1</b> Audience</a></li>
<li class="chapter" data-level="0.3.2" data-path="index.html"><a href="index.html#general-style"><i class="fa fa-check"></i><b>0.3.2</b> General Style</a></li>
<li class="chapter" data-level="0.3.3" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i><b>0.3.3</b> Learning Objectives</a></li>
<li class="chapter" data-level="0.3.4" data-path="index.html"><a href="index.html#summary"><i class="fa fa-check"></i><b>0.3.4</b> Summary</a></li>
<li class="chapter" data-level="0.3.5" data-path="index.html"><a href="index.html#key-terms"><i class="fa fa-check"></i><b>0.3.5</b> Key Terms</a></li>
<li class="chapter" data-level="0.3.6" data-path="index.html"><a href="index.html#headings-and-labels"><i class="fa fa-check"></i><b>0.3.6</b> Headings and Labels</a></li>
<li class="chapter" data-level="0.3.7" data-path="index.html"><a href="index.html#formulae"><i class="fa fa-check"></i><b>0.3.7</b> Formulae</a></li>
<li class="chapter" data-level="0.3.8" data-path="index.html"><a href="index.html#units"><i class="fa fa-check"></i><b>0.3.8</b> Units</a></li>
<li class="chapter" data-level="0.3.9" data-path="index.html"><a href="index.html#numbers"><i class="fa fa-check"></i><b>0.3.9</b> Numbers</a></li>
<li class="chapter" data-level="0.3.10" data-path="index.html"><a href="index.html#dates-and-times"><i class="fa fa-check"></i><b>0.3.10</b> Dates and times</a></li>
<li class="chapter" data-level="0.3.11" data-path="index.html"><a href="index.html#tables"><i class="fa fa-check"></i><b>0.3.11</b> Tables</a></li>
<li class="chapter" data-level="0.3.12" data-path="index.html"><a href="index.html#code-blocks"><i class="fa fa-check"></i><b>0.3.12</b> Code blocks</a></li>
<li class="chapter" data-level="0.3.13" data-path="index.html"><a href="index.html#abbreviations"><i class="fa fa-check"></i><b>0.3.13</b> Abbreviations</a></li>
<li class="chapter" data-level="0.3.14" data-path="index.html"><a href="index.html#initialisms"><i class="fa fa-check"></i><b>0.3.14</b> Initialisms</a></li>
<li class="chapter" data-level="0.3.15" data-path="index.html"><a href="index.html#acronyms"><i class="fa fa-check"></i><b>0.3.15</b> Acronyms</a></li>
<li class="chapter" data-level="0.3.16" data-path="index.html"><a href="index.html#punctuation"><i class="fa fa-check"></i><b>0.3.16</b> Punctuation</a></li>
<li class="chapter" data-level="0.3.17" data-path="index.html"><a href="index.html#citations"><i class="fa fa-check"></i><b>0.3.17</b> Citations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chapter-template.html"><a href="chapter-template.html"><i class="fa fa-check"></i><b>1</b> Chapter Title</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-template.html"><a href="chapter-template.html#key-terms-1"><i class="fa fa-check"></i>Key Terms</a></li>
<li class="chapter" data-level="1.1" data-path="chapter-template.html"><a href="chapter-template.html#first-section-header"><i class="fa fa-check"></i><b>1.1</b> First Section Header</a></li>
<li class="chapter" data-level="1.2" data-path="chapter-template.html"><a href="chapter-template.html#second-section-header"><i class="fa fa-check"></i><b>1.2</b> Second Section Header</a></li>
<li class="chapter" data-level="1.3" data-path="chapter-template.html"><a href="chapter-template.html#third-section-header"><i class="fa fa-check"></i><b>1.3</b> Third Section Header</a></li>
<li class="chapter" data-level="1.4" data-path="chapter-template.html"><a href="chapter-template.html#summary-1"><i class="fa fa-check"></i><b>1.4</b> Summary</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-template.html"><a href="chapter-template.html#reflection-questions"><i class="fa fa-check"></i>Reflection Questions</a></li>
<li class="chapter" data-level="" data-path="chapter-template.html"><a href="chapter-template.html#practice-questions"><i class="fa fa-check"></i>Practice Questions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chapter-template.html"><a href="chapter-template.html#recommended-readings"><i class="fa fa-check"></i>Recommended Readings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mapping-data.html"><a href="mapping-data.html"><i class="fa fa-check"></i><b>2</b> Mapping Data</a>
<ul>
<li class="chapter" data-level="" data-path="mapping-data.html"><a href="mapping-data.html#key-terms-2"><i class="fa fa-check"></i>Key Terms</a></li>
<li class="chapter" data-level="2.1" data-path="mapping-data.html"><a href="mapping-data.html#introduction-to-geodesy"><i class="fa fa-check"></i><b>2.1</b> Introduction to geodesy</a></li>
<li class="chapter" data-level="2.2" data-path="mapping-data.html"><a href="mapping-data.html#models-of-earth"><i class="fa fa-check"></i><b>2.2</b> Models of Earth</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="mapping-data.html"><a href="mapping-data.html#geodetic-vertical-datums"><i class="fa fa-check"></i><b>2.2.1</b> Geodetic vertical datums</a></li>
<li class="chapter" data-level="2.2.2" data-path="mapping-data.html"><a href="mapping-data.html#tidal-vertical-datums"><i class="fa fa-check"></i><b>2.2.2</b> Tidal Vertical Datums</a></li>
<li class="chapter" data-level="2.2.3" data-path="mapping-data.html"><a href="mapping-data.html#gravimetric-vertical-datums"><i class="fa fa-check"></i><b>2.2.3</b> Gravimetric Vertical Datums</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="mapping-data.html"><a href="mapping-data.html#referencing-location"><i class="fa fa-check"></i><b>2.3</b> Referencing Location</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="mapping-data.html"><a href="mapping-data.html#cartesian-coordinate-systems"><i class="fa fa-check"></i><b>2.3.1</b> Cartesian Coordinate Systems</a></li>
<li class="chapter" data-level="2.3.2" data-path="mapping-data.html"><a href="mapping-data.html#celestial-coordinate-systems"><i class="fa fa-check"></i><b>2.3.2</b> Celestial Coordinate Systems</a></li>
<li class="chapter" data-level="2.3.3" data-path="mapping-data.html"><a href="mapping-data.html#geographic-coordinate-systems"><i class="fa fa-check"></i><b>2.3.3</b> Geographic Coordinate Systems</a></li>
<li class="chapter" data-level="2.3.4" data-path="mapping-data.html"><a href="mapping-data.html#projected-coordinate-systems"><i class="fa fa-check"></i><b>2.3.4</b> Projected Coordinate Systems</a></li>
<li class="chapter" data-level="2.3.5" data-path="mapping-data.html"><a href="mapping-data.html#measuring-map-projection-distortion"><i class="fa fa-check"></i><b>2.3.5</b> Measuring map projection distortion</a></li>
<li class="chapter" data-level="2.3.6" data-path="mapping-data.html"><a href="mapping-data.html#map-projections-for-environmental-management"><i class="fa fa-check"></i><b>2.3.6</b> Map projections for environmental management</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="mapping-data.html"><a href="mapping-data.html#summary-2"><i class="fa fa-check"></i><b>2.4</b> Summary</a>
<ul>
<li class="chapter" data-level="" data-path="mapping-data.html"><a href="mapping-data.html#reflection-questions-1"><i class="fa fa-check"></i>Reflection Questions</a></li>
<li class="chapter" data-level="" data-path="mapping-data.html"><a href="mapping-data.html#practice-questions-1"><i class="fa fa-check"></i>Practice Questions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mapping-data.html"><a href="mapping-data.html#recommended-readings-1"><i class="fa fa-check"></i>Recommended Readings</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html"><i class="fa fa-check"></i><b>3</b> Fundamentals of remote sensing</a>
<ul>
<li class="chapter" data-level="" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#key-terms-3"><i class="fa fa-check"></i>Key Terms</a></li>
<li class="chapter" data-level="3.1" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#what-is-remote-sensing"><i class="fa fa-check"></i><b>3.1</b> What is remote sensing?</a></li>
<li class="chapter" data-level="3.2" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#types-of-energy"><i class="fa fa-check"></i><b>3.2</b> Types of Energy</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#introduction"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#scientific-notation"><i class="fa fa-check"></i><b>3.2.2</b> Scientific Notation</a></li>
<li class="chapter" data-level="3.2.3" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#electromagnetic-spectrum"><i class="fa fa-check"></i><b>3.2.3</b> Electromagnetic Spectrum</a></li>
<li class="chapter" data-level="3.2.4" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#radiation-types"><i class="fa fa-check"></i><b>3.2.4</b> Radiation Types</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#physical-laws-of-radiation"><i class="fa fa-check"></i><b>3.3</b> Physical laws of radiation</a></li>
<li class="chapter" data-level="3.4" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#the-four-resolutions"><i class="fa fa-check"></i><b>3.4</b> The Four Resolutions</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#spatial-resolution"><i class="fa fa-check"></i><b>3.4.1</b> Spatial Resolution</a></li>
<li class="chapter" data-level="3.4.2" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#temporal-resolution"><i class="fa fa-check"></i><b>3.4.2</b> Temporal Resolution</a></li>
<li class="chapter" data-level="3.4.3" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#spectral-resolution"><i class="fa fa-check"></i><b>3.4.3</b> Spectral Resolution</a></li>
<li class="chapter" data-level="3.4.4" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#radiometric-resolution"><i class="fa fa-check"></i><b>3.4.4</b> Radiometric Resolution</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#key-applications"><i class="fa fa-check"></i><b>3.5</b> Key Applications</a></li>
<li class="chapter" data-level="3.6" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#summary-3"><i class="fa fa-check"></i><b>3.6</b> Summary</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#reflection-questions-2"><i class="fa fa-check"></i><b>3.6.1</b> Reflection Questions</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="fundamentals-of-remote-sensing.html"><a href="fundamentals-of-remote-sensing.html#section"><i class="fa fa-check"></i><b>3.7</b> —</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="image-processing.html"><a href="image-processing.html"><i class="fa fa-check"></i><b>4</b> Image Processing</a>
<ul>
<li class="chapter" data-level="" data-path="image-processing.html"><a href="image-processing.html#key-terms-4"><i class="fa fa-check"></i>Key Terms</a></li>
<li class="chapter" data-level="4.1" data-path="image-processing.html"><a href="image-processing.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="image-processing.html"><a href="image-processing.html#geometric-correction"><i class="fa fa-check"></i><b>4.2</b> Geometric correction</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="image-processing.html"><a href="image-processing.html#relief-displacement"><i class="fa fa-check"></i><b>4.2.1</b> <strong>Relief displacement</strong></a></li>
<li class="chapter" data-level="4.2.2" data-path="image-processing.html"><a href="image-processing.html#georeferencing"><i class="fa fa-check"></i><b>4.2.2</b> <strong>Georeferencing</strong></a></li>
<li class="chapter" data-level="4.2.3" data-path="image-processing.html"><a href="image-processing.html#georegistration-georectification"><i class="fa fa-check"></i><b>4.2.3</b> <strong>Georegistration (georectification)</strong></a></li>
<li class="chapter" data-level="4.2.4" data-path="image-processing.html"><a href="image-processing.html#resampling"><i class="fa fa-check"></i><b>4.2.4</b> <strong>Resampling</strong></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="image-processing.html"><a href="image-processing.html#atmospheric-correction"><i class="fa fa-check"></i><b>4.3</b> Atmospheric correction</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="image-processing.html"><a href="image-processing.html#atmospheric-windows"><i class="fa fa-check"></i><b>4.3.1</b> <strong>Atmospheric windows</strong></a></li>
<li class="chapter" data-level="4.3.2" data-path="image-processing.html"><a href="image-processing.html#clouds-and-shadows"><i class="fa fa-check"></i><b>4.3.2</b> <strong>Clouds and shadows</strong></a></li>
<li class="chapter" data-level="4.3.3" data-path="image-processing.html"><a href="image-processing.html#smoke-and-haze"><i class="fa fa-check"></i><b>4.3.3</b> <strong>Smoke and Haze</strong></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="image-processing.html"><a href="image-processing.html#radiometric-correction"><i class="fa fa-check"></i><b>4.4</b> Radiometric correction</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="image-processing.html"><a href="image-processing.html#signal-to-noise"><i class="fa fa-check"></i><b>4.4.1</b> <strong>Signal-to-noise</strong></a></li>
<li class="chapter" data-level="4.4.2" data-path="image-processing.html"><a href="image-processing.html#radiometric-normalization"><i class="fa fa-check"></i><b>4.4.2</b> <strong>Radiometric normalization</strong></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="image-processing.html"><a href="image-processing.html#image-enhancement"><i class="fa fa-check"></i><b>4.5</b> Image enhancement</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="image-processing.html"><a href="image-processing.html#stretching"><i class="fa fa-check"></i><b>4.5.1</b> <strong>Stretching</strong></a></li>
<li class="chapter" data-level="4.5.2" data-path="image-processing.html"><a href="image-processing.html#sharpening"><i class="fa fa-check"></i><b>4.5.2</b> <strong>Sharpening</strong></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="image-processing.html"><a href="image-processing.html#summary-4"><i class="fa fa-check"></i><b>4.6</b> Summary</a>
<ul>
<li class="chapter" data-level="" data-path="image-processing.html"><a href="image-processing.html#reflection-questions-3"><i class="fa fa-check"></i>Reflection Questions</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Open Geomatics Textbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="image-processing" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Image Processing</h1>
<p>The collection of imagery is challenging and time consuming, with sensor design and deployment often requiring the development of new technologies. Driven by the growing demand for relevant information about environmental change, great emphasis is placed on the creation of “the next new satellite” or “a smaller, yet more powerful drone.” Although innovation in these physical technologies is critical for the advancement of image collection, they don’t guarantee that the imagery will be useful. In reality, the pixel values collected by a sensor are not purely the reflectance values from the surface of interest. Depending on the wavelengths being observed, there may be variations in the amount of photons emitted from the light source and a variety of atmospheric effects, such as scattering. There may also be slight inconsistencies in images collected by the same sensor on the same day or adjacent areas, which would affect the quality of analyses.</p>
<div class="box-content learning-objectives-content">
<div id="learning-objectives-4" class="section level4 unnumbered box-title learning-objectives-top">
<h4>Learning Objectives</h4>
</div>
<ol style="list-style-type: decimal">
<li>Relate common issues of image collection to relevant image processing techniques</li>
<li>Understand the logic supporting the use of specific image processing techniques</li>
<li>Explore a variety of processing methodologies employed by published research.</li>
</ol>
</div>
<div id="key-terms-4" class="section level3 unnumbered">
<h3>Key Terms</h3>
<p>pixel, spatial resolution, temporal resolution, radiometric resolution, spectral resolution, geometric correction, atmospheric correction,</p>
</div>
<div id="overview" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Overview</h2>
<p>The application of correction methods that address these inconsistencies are often described generally as “image processing.” In this chapter we will explore a variety of common image processing techniques and strive to understand the logic behind employing one, or more, to remote sensed imagery. Before diving into specific processing workflows that render imagery scientifically useful, however, it is important to review some key terms.</p>
<p>First and foremost, <em>image processing,</em> or <em>digital image analysis,</em> refers to any actions taken to improve the accuracy of one or more component of raw imagery. In remote sensing science, the goal of image processing is to generate a <em>product</em> that provides accurate and useful information for scientific pursuit. This is in contrast to image processing for artistic purposes, which could include many similar steps, but focus on generating a product that is visually appealing.</p>
<p><em>Noise</em> is another common term associated with image processing and it refers to any element of the data that is not wanted. There are a variety of noise types, which we will discuss later. In contrast to noise, <em>signal</em> describes wanted components of the imagery. Combined, signal and noise provide guidance on what specific steps should be taken in relation to the data during image processing. Before they are addressed, however, it is also important to confirm the spatial accuracy of the data.</p>
<p>It is also important to review the elements of an <em>image</em>, or <em>raster</em>. The terms image and raster are interchangeable and refer to the a collection of spatial adjacent <em>cells</em>, or <em>pixel</em>. The terms cell and pixel are also interchangeable. An empty raster, or image, would contain pixels with no information. To say an image has been collected would simply mean that a sensor has collected information and stored that information in adjacent pixels. Although this seems straight forward there are a multitude of environmental and engineering factors that can affect the accurate collection of information. It is these confounding factors that image processing attempts to resolve in hopes of generating a data product that can be compared across space and time.</p>
</div>
<div id="geometric-correction" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Geometric correction</h2>
<p>By definition, remotely sensed data is collected by a sensor at some distance apart from the object(s) being observed. In many instances, the sensor is in motion, and at the very least it is subject to influence from environmental actors like wind. Interactions with phenomena like wind can cause the instantaneous field of view (IFOV) to move slightly during data collection, introducing spatial errors to the imagery. On top of issues relating to sensor displacement, sensors in motion may also observe adjacent areas with different topographic properties.</p>
<p>A combination of these two effects could be visualized by imagining an airplane flying over a forested hillside. As the sensor collects data the plane can be buffeted with wind, changing the direction of a sensor’s IFOV to an location that is not the original target. On top of issues with sensor movement, the elevation of the ground is constantly undulating, altering the distance at which the sensors observes the landscape below. These two issues that affect the spatial components of image collection compromise the accuracy of an image and need to be corrected. The general term used to refer to the spatial correction of an image is geometric correction.</p>
<div id="relief-displacement" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> <strong>Relief displacement</strong></h3>
<p>The first component of geometric correction we will discuss deals with the terrain being observed. Changes in the terrain over which an image is collected lead to inconsistencies in the distances at which information is collected. These differences lead to objects in the image appearing in location inaccurate with reality and can be rectified using the spatial information of the sensor, datum and object. Examples of this effect can be observed in any photograph containing tall structures, which would appear to be leaning outward from the center of the image, or principal point. The rectification of relief displacement can be represented by the equation:</p>
<span class="math display" id="eq:ortho">\[\begin{equation}
d = rh/H
\tag{4.1}
\end{equation}\]</span>
<p>where d = relief displacement, r = distance from the principal point to the image point of interest, h = difference in height between the datum and the point of interest and H = the height of the sensor above the datum.</p>
<div class="box-content your-turn-content">
<div id="your-turn-4" class="section level4 unnumbered box-title your-turn-top">
<h4>Your turn!</h4>
</div>
<p>
<p>What is the image displacement of a pixel that is 0.5 mm from the principal point, 57 m below the datum, and collected from a sensor that is 135 m above the datum?</p>
</p>
</div>
</div>
<div id="georeferencing" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> <strong>Georeferencing</strong></h3>
<p>The removal of inaccuracies in the spatial location of an image can be conducted using a technique called georeferencing. The basic concept of georeferencing is to alter the coordinates of an image through the association of highly precise coordinates collected on site using a GPS. Coordinates collected in the field are often called ground, or control points, and form the base of successful georeferencing. In general, increasing the amount and accuracy of ground points leads to increased spatial accuracy of the image.</p>
<p>There are a variety of techniques used to transform the coordinates of an image based on control points, all of which are require some level of mathematics. The complexity of the polynomials used to transform the dataset, the more accurate the output spatial coordinates will be. Of course, the overall accuracy of the transformation depends on how accurate the control points are. Upon transforming a rasters coordinates it is important to evaluate how accurate the output raster is compared to the input raster. A common method of evaluating the success is through the calculating the square root of the mean of the square of all error, often called the Root Mean Squared Error (RMSE). In short, RMSE represents the average distance at which the output raster is from the ground or control points. The smaller the RMSE, the more accurate the transformation.</p>
</div>
<div id="georegistration-georectification" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> <strong>Georegistration (georectification)</strong></h3>
<p>Similar to georeferencing, georegistration involves adjusting the raw coordinates of an image to match more accurate ones. In the case of georegistration, however, ground points collected in the field are replaced with coordinates from a map or image that has been verified as spatially accurate. This method could be considered a matching of two products, enabling the two products to be analyzed together. An example would be matching two images collected one year apart. If the first image is georeferenced accurately the second image can simply be georegistered by identifying shared features, such as intersections or buildings, and linking them through the creation of control points in each image.</p>
</div>
<div id="resampling" class="section level3" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> <strong>Resampling</strong></h3>
<p>Despite all the efforts to accurately place an image in space, it is likely that any spatial alterations also change the shape or alignment of the image’s pixels. This dislocation between pixel sizes within the image, as well as potential changes in their directionality alter the capacity to evaluate the radiation values stored within them. Imagine that a raw image is represented by a table cloth. On top of the table cloth are thousands of pixels, all of uniform height and width. The corners of this table cloth each have X and Y coordinates. Now imagine that you have to match these coordinates (the corners) to the four more accurate ground points that don’t match the table cloth. Completion of this task requires you to stretch one corner of the table cloth outwards, while the other three corners are shifted inside, leading the table cloth to alter it’s shape and, in doing so, altering the location and shape of the raw pixels.</p>
<p>This lack of uniformity in pixel size and shape could compromise future image analysis and need to be corrected for. To do so, users can implement a variety of methods to reassign the spatially accurate values to spatially uniform pixels. This process of transforming an image from one set of coordinates to another is called resampling (Parker 1983 Zotero not working).</p>
<p>The most important concept to understand about resampling is also the first step in the process, which is the creation of a new, empty raster in which all cells are of equal size and are aligned North (Insert demo image). The transformed raster is overlaid with the empty raster with a user defined cell size before each empty cell is assigned a value based on values from the transformed raster. Confusing? Perhaps, but there are a variety of process that can be used to assign cell values to the empty raster and we will discuss three in hopes of clarifying this methodology.</p>
<p><strong>Nearest neighbor</strong></p>
<p>Nearest neighbor (NN) is the most simple method of resampling as it looks only at one pixel from the transformed raster. This pixel is selected based on the proximity of it’s center to the center of the empty cell and the value is added without further transformation. The simplicity of this method makes it excellent at preserving categorical data, like land cover or aspect, but struggles to capture transitions between cells and can result in output rasters that appear somewhat crude and blockish.</p>
<p><strong>Bilinear interpolation</strong></p>
<p>In contrast to NN resampling, bilinear interpolation (BI) uses the values of multiple neighboring cells in the transformed raster to determine the value for a single cell in the empty raster. Essentially, the four cells with centers nearest to the center of the empty cell are selected as input values. A weighted average of these four values is calculated based on their distance from the empty cell and this averaged value becomes the values of the empty cell. The process of calculating an average means that the output value is likely not the same as any of the input values, but remains within their range. These features make BI ideal for datasets with continuous variables, like elevation, rather than categorical ones.</p>
<p><strong>Cubic convolution</strong></p>
<p>Similar to bilinear interpolation, cubic convolution (CC) uses multiple cells in the transformed raster to generate the output for a single cell in the empty raster. Instead of using four neighbors, however, this method uses 16. The idea supporting the use of the 16 nearest neighbors is that it results in an output raster with cell values that are more similar to each other than the values of the input raster. This effect is called smoothing and is effective at removing noise, which makes CC the ideal sampling method for imagery. There is one drawback of this smoothing effect, however, as the output value of a cell may be outside the range of the 16 input cell values.</p>
<div class="figure"><span id="fig:13-cubic"></span>
<img src="images/13-cubic_convolution.PNG" alt="Demonstration of neighbour selection (red) using cubic convolution resampling to determine the value of a single cell (yellow) in an empty raster." width="100%" />
<p class="caption">
Figure 4.1: Demonstration of neighbour selection (red) using cubic convolution resampling to determine the value of a single cell (yellow) in an empty raster.
</p>
</div>
</div>
</div>
<div id="atmospheric-correction" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Atmospheric correction</h2>
<p>Following similar logic to that promoting the need for geometric correction, atmospheric correction is intended to minimize discrepancies in pixel values within and across images that occur due to interactions between observed radiation and atmosphere. The severity of impact that the atmosphere has on an image relates to the amount of heterogeneity that occurs in the atmosphere during image collection. The majority of impacts are caused by the three main types of scattering, which were presented in Chapter <a href="fundamentals-of-remote-sensing.html#fundamentals-of-remote-sensing">3</a>.</p>
<div id="atmospheric-windows" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> <strong>Atmospheric windows</strong></h3>
<p>A key characteristic of the earth’s atmosphere that impacts the collection of passive remotely sensed data is the impediment of certain wavelengths. If solar radiation of a specific wavelength cannot reach Earth’s surface, it is impossible for a sensor to detect the radiance of that wavelength. There are, however, certain regions of the electromagnetic spectrum (EMS), called atmospheric windows, that are less effected by absorption and scattering than others and it is the observation of these regions that remote sensing relies on. Upon reaching the Earth’s surface, however, there are a variety of atmospheric constituents that can affect image quality.</p>
</div>
<div id="clouds-and-shadows" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> <strong>Clouds and shadows</strong></h3>
<p>Two of the most common culprits in the disturbance of remotely sensed imagery are clouds and shadows. Both are relatively transient, making the prediction of their inclusion in an image difficult and rendering their effects within an image relatively inconsistent. On top of issues of presence, each introduces unique challenges for image correction.</p>
<p>Clouds are an inherent component of Earth’s atmosphere and therefore should warrant respect and care in image processing, rather than sighs of frustration. You could imagine that a single image with 30% cloud cover may not be entirely useful, but their aforementioned permanent transience means that data users must work to reduce their effects, if not remove them entirely.</p>
<p>When approaching the removal of clouds, it is important to recall the physics that drive Mie scattering (Chapter <a href="fundamentals-of-remote-sensing.html#fundamentals-of-remote-sensing">3</a>. Essentially, water vapors in the atmosphere scatter visible and near infrared light and generate what appears to be white objects in the sky. Since the visible and near infrared regions of the EMS fall within an atmospheric window in which many sensor detect radiation, clouds can be recorded as part of an image.</p>
<p>The removal of clouds is often referred to as masking and can prove challenging depending on the region in question as they also generate cloud-shadows. You could imagine a study attempting to evaluate snow cover over a landscape using imagery comprised of wavelengths in the visible region of the EMS. If there was intermittent cloud cover, cloudy areas with no snow could be classified as having snow and snowy areas with cloud-shadows may be classified as no snow.</p>
<p>Martinuzzi et al 2007 published a masking method to remove cloud and cloud-shadow from Landsat imagery . They proposed the utilization of radiation values collected in the blue (Band 1) and thermal (Band 6) ranges of the EMS to differentiate clouds from objects on the Earth’s surface. To remove cloud-shadow, they suggested using information collected from Band 4, in the near infrared region, combined with the predicted location of shadows based on the spatial relationships of the cloud, sun and senor.</p>
<p>Although Martinuzzi et al. 2007 presented a straightforward method for making cloud and cloud-shadow in an image, they did not address the inclusion of shadows caused by landcover structures, such as trees or buildings <span class="citation">(<a href="#ref-martinuzzi2007" role="doc-biblioref">Martinuzzi, Gould, and Gonzalez 2007</a>)</span>. Shadows present unique problems for image analysis as they can shade out underlying structures and also be classified as seperate, individual objects. The former issues presents problems for studies evaluating land cover, while the latter confounds machine learning algorithms attempting to identify unique classes in the image based on spectral similarities. Another confounding issue is that the location and size of shadows change throughout the day in accordance with the sun.</p>
<p>An important feature of any shadow is that the area shaded is still considered to be illuminated, but only by skylight . The exclusion of sunlight from the area creates a unique opportunity for shadow identification and removal <span class="citation">(<a href="#ref-finlayson2001" role="doc-biblioref">Finlayson and Hordley 2001</a>)</span>. Finlayson et al.’s method of shadow removal is complex, using derivative calculus to capitalize on the fact that a illumination invariant function can be recognized based solely on surface reflectance. Although more complicated that Martinuzzi et al.’s approach, it may be worth reviewing <span class="citation">(<a href="#ref-finlayson2001" role="doc-biblioref">Finlayson and Hordley 2001</a>)</span> if you are interested in learning more about shadow removal.</p>
</div>
<div id="smoke-and-haze" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> <strong>Smoke and Haze</strong></h3>
<p>Smoke and haze present unique issues to image processing as they tend to vary in presence, consistency and density. They also represent different types of scattering, with smoke causing Mie scattering and haze causing non-selective scattering. Makarau et al., 2014 demonstrated that haze can be removed through the creation of a haze thickness map (HTM) <span class="citation">(<a href="#ref-makarau2014" role="doc-biblioref">Makarau et al. 2014</a>)</span>. This methodology is equally as complex as that of <span class="citation">(<a href="#ref-finlayson2001" role="doc-biblioref">Finlayson and Hordley 2001</a>)</span> and is perhaps beyond the scope of this book. It is important to note, though, that removal of shadows, clouds, smoke and haze relies on an understanding of how their respective scattering types affect incoming solar radiation. Successful removal, then, depends on using spectral bands that are not effected by the particular type of scattering occurring within the image.</p>
</div>
</div>
<div id="radiometric-correction" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Radiometric correction</h2>
<p>We have discussed how the creation of an image by a remote sensor leads to slight variations in spatial and atmospheric properties between pixels and that these inconsistencies must be corrected for. In this section, we will discuss some issues affecting the information within a pixel and some common remedies. In essence, we will explore how the raw digital numbers collected by a sensor can be converted to radiance and reflectance.</p>
<div id="signal-to-noise" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> <strong>Signal-to-noise</strong></h3>
<p>A key concept of radiometric correction is the ratio of desired information, or signal to background information (noise) within a pixel. The signal-to-noise ratio (SNR) is a common method of presenting this information and provides an overall statement about image quality. A common method of calculating SNR is to divide the mean (µ) signal value of the sensor by its standard deviation (𝛔), where signal represents an optical intensity. (Equation <a href="#eq:snr">(4.2)</a>)</p>
<span class="math display" id="eq:snr">\[\begin{equation}
SNR = µ _{signal} /𝛔 _{signal}
\tag{4.2}
\end{equation}\]</span>
<p>It is clear, through Equation <a href="image-processing.html#eq:snr">(4.2)</a>, that the average signal value of an instrument represents the value that its designers desire to capture. It is also clear that an increase in signal leads to an improved SNR. What remains unclear, however, is what causes a sensor to observe and record undesired noise to be recorded. In reality, there are a variety of noise types that can affect the SNR of a sensor.</p>
<p><strong>Photon shot noise</strong></p>
<p>The first type of noise we will address is due to inconsistencies in the number of photons observed at different time intervals. Since photons are quantum particles (they are the smallest measurable amount of radiation), a sensor can only observe them as whole numbers. This, coupled with other random fluctuations in radiation properties, means that a sensor directed at the same location may observe 10 photons at one time interval and 12 a later interval. Over time, it is possible to determine the average amount of photons observed by a sensor, but creates inconsistencies across individual observations. Photon shot noise can be calculated as the square root, of the signal. This suggests that stronger signal leads to a relative decrease in shot noise.</p>
<p><strong>Pattern noise</strong></p>
<p>Also known as fixed-pattern noise, pattern noise arises in instruments that have multiple sensors along a single array. Pattern noise occurs due to one sensor collecting relatively higher values than others, resulting in heightened brightness in the pixels it creates.</p>
<p><strong>Readout noise</strong></p>
<p>Readout noise is created through the inconsistencies relating to the interaction of multiple physical measurement electronic devices. Since it is impossible to have a sensor without physical devices, readout noise is inherent in all sensors. Readout noise is therefore equal to any difference in pixel value when all sensors are exposed to identical levels of illumination. There are a variety of technical methods used to correct for this error, but the concepts and mathematics supporting them are perhaps beyond the scope of this book. If you are interested in learning more about readout noise, check out this <a href="http://spiff.rit.edu/classes/phys445/lectures/readout/readout.html" title="Readout Noise by Michael Richmond">webpage</a> created by Michael Richmond.</p>
<p><strong>Thermal noise</strong></p>
<p>Another inherent type of sensor noise is thermal noise. Thermal noise occurs in any device using electricity and is caused by the vibrations of the devices charge carriers. This means that thermal noise can never fully be removed from an image, although it can be reduced by lowering the temperature of the environment at which the sensor is operating.</p>
<div class="box-content your-turn-content">
<div id="your-turn-5" class="section level4 unnumbered box-title your-turn-top">
<h4>Your turn!</h4>
</div>
<p>Calculate SNR or it’s associated values for various Landsat sensors:</p>
<ol style="list-style-type: decimal">
<li><p>OLI = signal 5288.1, standard deviation - 18.7</p></li>
<li><p>TM = 𝛔: 0.4, µ: 5.8</p></li>
<li><p>ETM+ = SNR: 22.3, µ: 13.4</p></li>
</ol>
</div>
</div>
<div id="radiometric-normalization" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> <strong>Radiometric normalization</strong></h3>
<p>As discussed in Chapter <a href="fundamentals-of-remote-sensing.html#fundamentals-of-remote-sensing">3</a>, the normalization of radiometric values collected by a sensor enables the comparison of data within and across images. A common normalization output in remote sensing studies evaluating the environment is reflectance, which can be calculated simply by dividing the radiance value of a pixel collected when observing the object(s) of interest with the radiance value of a pixel observing a surface with 100% reflectance.</p>
<div class="box-content case-study-content">
<div id="case-study-walk-through-the-steps-that-generate-landsat-tiers" class="section level4 unnumbered box-title case-study-top">
<h4>Case Study: Walk through the steps that generate Landsat tiers</h4>
</div>
<div id="box-text" class="section level4 unnumbered">
<h4>Case study title max of forty characters</h4>
<p>
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent non magna non nunc auctor eleifend. Etiam fringilla fermentum nisl at volutpat. Duis sodales lorem interdum, posuere neque sollicitudin, malesuada nisl. Sed laoreet commodo dui et cursus. Curabitur vel porta mauris. In lacinia ac ex ac varius. Nullam at vulputate ligula. Nunc sed faucibus urna, eget placerat sapien. Fusce ut lorem a ante congue consequat et sed tortor. Integer neque urna, vehicula at aliquam non, luctus non mi. Pellentesque et massa vehicula, cursus erat id, aliquet sapien. Sed rhoncus vehicula risus, vel aliquam eros porta eget. Etiam maximus, massa in pretium semper, est libero feugiat ipsum, quis tempor nunc libero in nibh. Nunc sollicitudin mattis metus ut rutrum. Donec urna purus, sodales id nibh in, ultricies tincidunt nibh. Proin porta accumsan aliquet.</p>
</p>
<p>
<p>Cras convallis erat ante, ut tristique est tempor elementum. Nam mollis, ipsum at vehicula vestibulum, est magna finibus nisl, et laoreet metus massa et ipsum. Proin eget eros ac odio euismod volutpat et ac diam. Cras viverra ut libero vel pulvinar. Duis nisi magna, sagittis id ligula ac, efficitur commodo eros. Nulla tincidunt id nulla in lobortis. Sed non mi eu mi fermentum cursus. In elit velit, semper sed gravida at, imperdiet sed nibh. Aliquam quis massa malesuada, venenatis nunc sed, malesuada nulla. Vestibulum malesuada purus ut ex ullamcorper, ut blandit lacus lobortis. Curabitur scelerisque velit justo, quis porttitor purus efficitur ut. Phasellus nec arcu vestibulum, consequat ante id, elementum velit. Proin arcu tortor, cursus vitae sem id, congue semper urna. Integer tempus in est eu consequat. Donec sodales, quam vel finibus faucibus, leo ante dictum quam, id tempus ligula dolor quis erat. Curabitur non elementum sem. Mauris placerat fermentum orci non lacinia. Ut imperdiet dui lectus, ac malesuada felis euismod sed. Nulla non volutpat dui, in suscipit turpis.</p>
</p>
<p>
<p>Case studies should have at least one image or map (no more than 2 total) and the written length should be around 300 words (shown above). Any references to external literature should by hyperlinked with the Digitial Object Identifier (DOI) permanent URL and <a href="https://bookdown.org/yihui/bookdown/citations.html" target="_blank">entered into the bibliography</a>. Avoid linking to external resources without a DOI and permanent URL. Contact Paul or try using the Leaflet package in R if you want to add an interactive web map.</p>
</p>
</div>
</div>
</div>
</div>
<div id="image-enhancement" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Image enhancement</h2>
<p>So far, in this Chapter, we have discussed methods of correcting spatial, atmospheric and radiometric errors that are commonly present in remotely sensed images. While the removal of these artifacts is necessary, it is also important to explore some common methods of enhancing an image once these aforementioned corrections have been made. Both image stretching and sharpening have roots in spatial and radiometric correction, so keep your mind open to the inherent links that arise.</p>
<div id="stretching" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> <strong>Stretching</strong></h3>
<p>Image stretching refers to the adjustment of radiometric values of the input methods to better exploit the radiometric resolution of an image. In principle, the distribution of radiometric values within a image is altered in a manner that improves it’s capacity to perform a desired task (). For instance, if an image collected with an 8-bit radiometric resolution (256 radiometric values; 0-255). appears too dark, it is likely that the distribution of pixel values is centered on a radiometric value greater than 127 (middle of a 8-bit scale). In fact, of 0 represents white and 255 represents black, it is very likely that the majority of pixels are closer to 255. In this case, the lowest value(s) observed in the image can be adjusted to 0, the relationship of this change can be determined and then applied to all other observed values.</p>
<div class="figure"><span id="fig:13-stretch"></span>
<img src="images/13-stretch.PNG" alt="Example of how (a) the original distribution of radiometric values in a image is (b) stretched" width="100%" />
<p class="caption">
Figure 4.2: Example of how (a) the original distribution of radiometric values in a image is (b) stretched
</p>
</div>
</div>
<div id="sharpening" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> <strong>Sharpening</strong></h3>
<p>Image sharpening is often used to preserve the spectral information of a pixel while improving it’s spatial resolution <span class="citation">(<a href="#ref-king" role="doc-biblioref">King and Jianwen Wang, n.d.</a>)</span>. This is particularly useful for applications concerned with classification, such as land cover evaluation, as transitions between pixels with different values become more clear. For example, there are a variety of studies demonstrating that Landsat 8 imagery can be sharpened using information collected by a panchromatic sensor operating in unison with the other multispectral sensors on board the satellite <span class="citation">(<a href="#ref-gilbertson2017" role="doc-biblioref">Gilbertson, Kemp, and van Niekerk 2017</a>)</span>. This is made possible because the spatial resolution of the panchromatic sensor is 15 m-2, while the multispectral sensors collect information at 30 m-2.</p>
<div class="box-content call-out-content">
<div id="call-out-2" class="section level4 unnumbered box-title call-out-top">
<h4>Call out</h4>
</div>
<p>
<p>If you are interested in learning more about how sharpening can impact hyperspectral imagery you can check out <span class="citation">(<a href="#ref-inamdar2020" role="doc-biblioref">Inamdar et al. 2020</a>)</span>. Their research demonstrates that recorded pixel values contain information from areas beyond the traditional spatial boundary of a cell. These findings have interesting implications for a variety of applications.</p>
</p>
</div>
</div>
</div>
<div id="summary-4" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Summary</h2>
<p>This chapter provided an overview of common image processing techniques and discussed the logic that supports their usage. Overall, each technique strives to create imagery that is consistent across time and space in order for individual pixel values to be evaluated and/or compared. Although necessary, these processes can take time and need to be applied in accordance with the desired application. Understanding the general workflow of image processing will allow you to determine what steps should be taken to create the highest quality imagery for your research.</p>
<div id="reflection-questions-3" class="section level3 unnumbered">
<h3>Reflection Questions</h3>
<ol style="list-style-type: decimal">
<li>Explain ipsum lorem.</li>
<li>Define ipsum lorem.</li>
<li>What is the role of ispum lorem?</li>
<li>How does ipsum lorem work?</li>
</ol>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Finlayson, Graham D., and Steven D. Hordley. 2001. <span>“Color Constancy at a Pixel.”</span> <em>Journal of the Optical Society of America A</em> 18 (2): 253. <a href="https://doi.org/10.1364/josaa.18.000253">https://doi.org/10.1364/josaa.18.000253</a>.
</div>
<div class="csl-entry">
Gilbertson, Jason Kane, Jaco Kemp, and Adriaan van Niekerk. 2017. <span>“Effect of Pan-Sharpening Multi-Temporal Landsat 8 Imagery for Crop Type Differentiation Using Different Classification Techniques.”</span> <em>Computers and Electronics in Agriculture</em> 134 (March): 151–59. <a href="https://doi.org/10.1016/j.compag.2016.12.006">https://doi.org/10.1016/j.compag.2016.12.006</a>.
</div>
<div class="csl-entry">
Inamdar, Deep, Margaret Kalacska, George Leblanc, and J Pablo Arroyo Mora. 2020. <span>“Characterizing and Mitigating Sensor Generated Spatial Correlations in Airborne Hyperspectral Imaging Data.”</span> <a href="https://doi.org/10.3390/rs12040641">https://doi.org/10.3390/rs12040641</a>.
</div>
<div class="csl-entry">
King, R.L., and Jianwen Wang. n.d. <span>“IGARSS 2001. Scanning the Present and Resolving the Future. Proceedings. IEEE 2001 International Geoscience and Remote Sensing Symposium.”</span> In. IEEE. <a href="https://doi.org/10.1109/igarss.2001.976657">https://doi.org/10.1109/igarss.2001.976657</a>.
</div>
<div class="csl-entry">
Makarau, Aliaksei, Rudolf Richter, Rupert Müller, and Peter Reinartz. 2014. <span>“Haze Detection and Removal in Remotely Sensed Multispectral Imagery.”</span> <em>IEEE Transactions on Geoscience and Remote Sensing</em> 52 (9): 5895–905. <a href="https://doi.org/10.1109/TGRS.2013.2293662">https://doi.org/10.1109/TGRS.2013.2293662</a>.
</div>
<div class="csl-entry">
Martinuzzi, Sebastián, William A. Gould, and Olga M. Ramos Gonzalez. 2007. <span>“Creating Cloud-Free Landsat ETM+ Data Sets in Tropical Landscapes: Cloud and Cloud-Shadow Removal.”</span> <em>U.S. Department of Agriculture, Forest Service, International Institute of Tropical Forestry. Gen. Tech. Rep. IITF-32.</em> 32. <a href="https://doi.org/10.2737/IITF-GTR-32">https://doi.org/10.2737/IITF-GTR-32</a>.
</div>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-finlayson2001" class="csl-entry">
Finlayson, Graham D., and Steven D. Hordley. 2001. <span>“Color Constancy at a Pixel.”</span> <em>Journal of the Optical Society of America A</em> 18 (2): 253. <a href="https://doi.org/10.1364/josaa.18.000253">https://doi.org/10.1364/josaa.18.000253</a>.
</div>
<div id="ref-gilbertson2017" class="csl-entry">
Gilbertson, Jason Kane, Jaco Kemp, and Adriaan van Niekerk. 2017. <span>“Effect of Pan-Sharpening Multi-Temporal Landsat 8 Imagery for Crop Type Differentiation Using Different Classification Techniques.”</span> <em>Computers and Electronics in Agriculture</em> 134 (March): 151–59. <a href="https://doi.org/10.1016/j.compag.2016.12.006">https://doi.org/10.1016/j.compag.2016.12.006</a>.
</div>
<div id="ref-inamdar2020" class="csl-entry">
Inamdar, Deep, Margaret Kalacska, George Leblanc, and J Pablo Arroyo Mora. 2020. <span>“Characterizing and Mitigating Sensor Generated Spatial Correlations in Airborne Hyperspectral Imaging Data.”</span> <a href="https://doi.org/10.3390/rs12040641">https://doi.org/10.3390/rs12040641</a>.
</div>
<div id="ref-king" class="csl-entry">
King, R.L., and Jianwen Wang. n.d. <span>“IGARSS 2001. Scanning the Present and Resolving the Future. Proceedings. IEEE 2001 International Geoscience and Remote Sensing Symposium.”</span> In. IEEE. <a href="https://doi.org/10.1109/igarss.2001.976657">https://doi.org/10.1109/igarss.2001.976657</a>.
</div>
<div id="ref-makarau2014" class="csl-entry">
Makarau, Aliaksei, Rudolf Richter, Rupert Müller, and Peter Reinartz. 2014. <span>“Haze Detection and Removal in Remotely Sensed Multispectral Imagery.”</span> <em>IEEE Transactions on Geoscience and Remote Sensing</em> 52 (9): 5895–905. <a href="https://doi.org/10.1109/TGRS.2013.2293662">https://doi.org/10.1109/TGRS.2013.2293662</a>.
</div>
<div id="ref-martinuzzi2007" class="csl-entry">
Martinuzzi, Sebastián, William A. Gould, and Olga M. Ramos Gonzalez. 2007. <span>“Creating Cloud-Free Landsat ETM+ Data Sets in Tropical Landscapes: Cloud and Cloud-Shadow Removal.”</span> <em>U.S. Department of Agriculture, Forest Service, International Institute of Tropical Forestry. Gen. Tech. Rep. IITF-32.</em> 32. <a href="https://doi.org/10.2737/IITF-GTR-32">https://doi.org/10.2737/IITF-GTR-32</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fundamentals-of-remote-sensing.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
